{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries Used for the Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine\n",
    "from bson import json_util, ObjectId\n",
    "from pandas.io.json import json_normalize\n",
    "from datetime import timedelta\n",
    "import calendar\n",
    "import json\n",
    "from bson import json_util, ObjectId\n",
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "connectToMongo - is a function to establish connection with mongoDB to extract data into python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def connectToMongo(hostname,port):\n",
    "    \"\"\"\n",
    "    This function establishes connection to Mongo DB and returns connection object\n",
    "    \"\"\"\n",
    "    mc = MongoClient(host='127.0.0.1',port=27017)\n",
    "    return mc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getcollection is a function to establish to apply find on mongodb to read data from the collections in mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCollection(database, collection, mongo_conn):\n",
    "    \"\"\"\n",
    "    This function gets data for Store Location\n",
    "    \"\"\"\n",
    "    db = mongo_conn.get_database(database)\n",
    "    lists = db.get_collection(collection).find({},{'_id':0})\n",
    "    return lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GetSalesTrx function to apply filters on the transaction date time and store number to filter specific set of sales records and extract data from mongodb collections and store it in python data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSalesTrx(sDate,eDate,store,database, collection, mongo_conn):\n",
    "    \"\"\"\n",
    "    This function gets data for Store Location\n",
    "    \"\"\"\n",
    "    db = mongo_conn.get_database(database)\n",
    "    lists = db.get_collection(collection).find({'StoreNum':store,'TransDatetime(Local)':{'$lt': eDate, '$gte': sDate}},{'_id':0})\n",
    "    return lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to DF function to convert the data extracted from mongo to data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToDF(lists):\n",
    "    \"\"\"\n",
    "    This function converts lists of data extracted from mongo to Data Frames.\n",
    "    \"\"\"\n",
    "    df= pd.DataFrame(list(lists))\n",
    "    print(\"Number of rows : \" + str(len(df)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to MySQL function to establish a connection with MySQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connectMySQL(username, password, hostname, db):\n",
    "    \"\"\"\n",
    "    This function estalishes connection to MySQL and returns connection object\n",
    "    username and password to be passed as pararmeters to access the database of chocie with hostname and databasename(db)\n",
    "    \"\"\"\n",
    "    #mysql+mysqlconnector://[username]:[password]@localhost/[database]\n",
    "    link = 'mysql+mysqlconnector://'+username+':'+password+'@'+hostname+'/'+db\n",
    "    engine = create_engine(link)\n",
    "    return engine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Housekeeping function to flush out old data in the data frames and tables and create a fresh set of inserts into the MySQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performHousekeeping(engine):\n",
    "    \"\"\"\n",
    "    This function deletes all data records from the database table\n",
    "    \"\"\"\n",
    "    list1 = ['DateDim','TimeDim','ItemListDim','ItemJunkDim','ItemHierarchyDim','StoreJunkDim',\n",
    "             'StoreLocationDim','SalesJunkDim','CustomerDim','ItemAttributesDim','StoreServiceDim',\n",
    "             'trans_fact']\n",
    "    for i in list1:\n",
    "        engine.execute('delete from '+i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to access the embedded documents within a document in mongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbedded(mongo_data):\n",
    "    \"\"\"\n",
    "    This function is to convert nested documents from mongoDB to DataFrames\n",
    "    \"\"\"\n",
    "    sanitized = json.loads(json_util.dumps(mongo_data))\n",
    "    normalized = json_normalize(sanitized)\n",
    "    df = pd.DataFrame(normalized)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a unknown customer to keep a record of the customers that do not have a loyalty card number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating dataframes for all the docuements from mongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    #Creating a mongoDB connection object\n",
    "    conn_obj = connectToMongo(hostname='127.0.0.1',port=27017)\n",
    "\n",
    "    unknowncustrecord = dict({\"LoyaltyCardNum\":-999,\"HouseholdNum\":-999,\"MemberFavStore\":-999,\"City\":'-999',\"State\":'-999',\"ZipCode\":'-999'})\n",
    "    unknowncustomer = conn_obj.get_database('BIProject').get_collection('Customer').insert_one(unknowncustrecord)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Get data from Mongo\n",
    "    \"\"\"\n",
    "    #Collect data from StoreLocation collection at mongoDB\n",
    "    storeDF = convertToDF(getCollection('BIProject','StoreLocation',conn_obj))\n",
    "    print(storeDF.dtypes)\n",
    "    print(storeDF.isna().sum())\n",
    "\n",
    "    #Collect data from ItemAttribute collection at mongoDB\n",
    "    itemAttrDF = convertToDF(getCollection('BIProject','ItemAttribute',conn_obj))\n",
    "    print(itemAttrDF.dtypes)\n",
    "    print(itemAttrDF.isna().sum())\n",
    "\n",
    "    #Collect data from ItemList collection at mongoDB\n",
    "    itemListDF = convertToDF(getCollection('BIProject','ItemList',conn_obj))\n",
    "    print(itemListDF.dtypes)\n",
    "    print(itemListDF.isna().sum())\n",
    "\n",
    "    #Collect data from Customer collection at mongoDB\n",
    "    customerDF = convertToDF(getCollection('BIProject','Customer',conn_obj))\n",
    "    print(customerDF.dtypes)\n",
    "    print(customerDF.isna().sum())\n",
    "\n",
    "    #Collect data from SalesTrx collection at mongoDB between specific transaction dates at a particular store\n",
    "    start = '2014-02-22 00:00:00'\n",
    "    end = '2014-02-23 00:00:00'\n",
    "    store = 562\n",
    "    saleDF = convertToDF(getSalesTrx(start, end,store,'BIProject','SalesTrx',conn_obj))\n",
    "    salejunkDF = saleDF[['StoreNum','Register','DeptNum','CashierNum','PriceType','ServiceType','TenderType']]\n",
    "    print(saleDF.dtypes)\n",
    "    print(saleDF.isna().sum())\n",
    "\n",
    "    #Extracting scraped data at mongoDB\n",
    "    DF = getEmbedded(getCollection('BIProject','StoreScraped', conn_obj))\n",
    "\n",
    "    scrapedDF = DF[['Service.Alcohol','Service.Amarillo National Bank','Service.Angus Beef',\n",
    "                  'Service.Bakery','Service.Bill Pay','Service.Boars Head','Service.Bulk Foods',\n",
    "                  'Service.Check Cashing','Service.City Bank','Service.Clear Talk','Service.Coffee Shop',\n",
    "                  'Service.Concierge','Service.DMV Registration','Service.Deli','Service.Dish Gift Center',\n",
    "                  'Service.First Financial Bank','Service.Floral','Service.Full Service Seafood','Service.Herring National Bank',\n",
    "                  'Service.Hot Deli','Service.Keva Juice','Service.Living Well Dept','Service.Lottery','Service.Meals For Two','Service.Meat Market',\n",
    "                  'Service.Red Box','Service.Restaurant','Service.Rug Doctor','Service.Salad Bar','Service.Sushi','Service.Team Spirit Shop','Service.Ticket Sales','Service.Walk-in Clinic',\n",
    "                  'Service.Wells Fargo Bank','Service.Western Union','StoreId','StoreName']]\n",
    "\n",
    "#renaming the scraped columns\n",
    "\n",
    "    scrapedDF.rename(columns={'Service.Alcohol':'Alcohol','Service.Amarillo National Bank':'AmarilloNationalBank','Service.Angus Beef':'AngusBeef',\n",
    "                  'Service.Bakery':'Bakery','Service.Bill Pay':'BillPay','Service.Boars Head':'BoarsHead','Service.Bulk Foods':'BulkFoods',\n",
    "                  'Service.Check Cashing':'CheckCashing','Service.City Bank':'CityBank','Service.Clear Talk':'ClearTalk','Service.Coffee Shop':'CoffeeShop',\n",
    "                  'Service.Concierge':'Concierge','Service.DMV Registration':'DMVregistration','Service.Deli':'Deli','Service.Dish Gift Center':'DishGiftCenter',\n",
    "                  'Service.First Financial Bank':'FirstFinancialBank','Service.Floral':'Floral','Service.Full Service Seafood':'FullServiceSeafood','Service.Herring National Bank':'HerringNationalBank',\n",
    "                  'Service.Hot Deli':'HotDeli','Service.Keva Juice':'KevaJuice','Service.Living Well Dept':'LivingWellDept','Service.Lottery':'Lottery','Service.Meals For Two':'MealsForTwo','Service.Meat Market':'MeatMarket',\n",
    "                  'Service.Red Box':'RedBox','Service.Restaurant':'Restaurant','Service.Rug Doctor':'RugDoctor','Service.Salad Bar':'SaladBar','Service.Sushi':'Sushi','Service.Team Spirit Shop':'TeamSpiritShop','Service.Ticket Sales':'TicketSales','Service.Walk-in Clinic':'WalkInClinic',\n",
    "                  'Service.Wells Fargo Bank':'WellsFargoBank','Service.Western Union':'WesternUnion','StoreId':'StoreNum','StoreName':'StoreType'},inplace=True)\n",
    "\n",
    "    scrapedDF.drop_duplicates(keep='first')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a mysql connection and insert the sales fact data as base for merging dimensional data with it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create alll dimension tables with auto increment so that dimension keys get autogenerated through auto increment column in the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge sales and customer data to categorise and link customers to the each sales transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    Create a mysql database connection\n",
    "    \"\"\"\n",
    "    engine = connectMySQL('[username]', '[password]', '[hostname]','[database name]')\n",
    "\n",
    "    #Delete all data from all tables in the database\n",
    "    performHousekeeping(engine)\n",
    "\n",
    "    \"\"\"\n",
    "    Inserting data values into Dimensions of the mysql database\n",
    "    \"\"\"\n",
    "\n",
    "    #Inserting values into sales junk dimension\n",
    "    #Merging customerDF with saleDF to get only those customers who are in our sales transaction file\n",
    "    cust_sale = pd.merge(customerDF, saleDF, left_on='LoyaltyCardNum', right_on='LoyaltyCardNumber',how = 'inner').drop_duplicates(keep='first')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inserting data into customerdim, storejunkdim and storelocationdim from the dataframes cust_Sale and storedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Inserting data into CustomerDim\n",
    "    CustomerDim = cust_sale[['LoyaltyCardNum','HouseholdNum','MemberFavStore','City','State','ZipCode']].drop_duplicates(keep='first').drop_duplicates(keep='first')\n",
    "    CustomerDim.to_sql('CustomerDim', engine, if_exists='append', index=False)\n",
    "\n",
    "    #Inserting data into StoreJunkDim\n",
    "    storeDF[['StoreNum','StoreName','ActiveFlag','SqFoot','ClusterName']].to_sql('StoreJunkDim', engine, if_exists='append', index=False)\n",
    "\n",
    "    #Inserting data into StoreLocationDim\n",
    "    StoreLocationDim = storeDF[['Region','StateCode','City','ZipCode','AddressLine1']].drop_duplicates(keep='first')\n",
    "    StoreLocationDim.to_sql('StoreLocationDim', engine, if_exists='append', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merged item information with sales data is loaded into all item related dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Merging itemListDF and saleDF to get only those items which are in our sales\n",
    "    item_sale = pd.merge(itemListDF, saleDF, left_on=['UPC','ItemID'], right_on=['UPC','ItemID'], how ='inner').drop_duplicates(keep='first')\n",
    "\n",
    "    #Inserting data into ItemListDim\n",
    "    ItemListDim = item_sale[['UPC','ItemID','LongDes','ShortDes','ExtraDes']].drop_duplicates(keep='first')\n",
    "    ItemListDim.to_sql('ItemListDim', engine, if_exists='append', index=False)\n",
    "\n",
    "    #Inserting data into item hierarchy\n",
    "    ItemHierarchyDim = item_sale[['DepartmentCode','FamilyCode','FamilyDes','CategoryCode','CategoryDes','ClassCode','ClassDes']].drop_duplicates(keep='last').astype(str).drop_duplicates(keep='first')\n",
    "    ItemHierarchyDim.to_sql('ItemHierarchyDim', engine, if_exists='append', index=False)\n",
    "\n",
    "    #Inserting data into ItemJunkDim\n",
    "    ItemJunkDim = item_sale[['StoreBrand','Status']].drop_duplicates(keep='first').drop_duplicates(keep='first')\n",
    "    ItemJunkDim.to_sql('ItemJunkDim', engine, if_exists='append', index=False)\n",
    "\n",
    "    #Inserting into scraped StoreServicesDim\n",
    "    scrapedDF.to_sql('StoreServiceDim', engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a item attribute outrigger dimension linked to item dimesion through bridge table item_Bridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Inserting into item attributes dimension\n",
    "    itemattrinitem = pd.merge(item_sale, itemAttrDF, left_on=['UPC'], right_on=['UPC'], how ='inner').drop_duplicates(keep='first')\n",
    "    ItemattributesDim = itemattrinitem[['UPC','ItemAttributeValue','ItemAttributeDes','AttributeStartDate','AttributeEndDate']]\n",
    "    ItemattributesDim.to_sql('ItemattributesDim', engine, if_exists='append', index=False)\n",
    "\n",
    "    #Inserting into ItemBridge table\n",
    "    itemdim = pd.read_sql_table('itemlistdim', engine, columns=['ILDK', 'UPC'])\n",
    "    itemattrdim = pd.read_sql_table('itemattributesdim', engine, columns=['IADK', 'UPC'])\n",
    "    itembridgedf = pd.merge(itemdim, itemattrdim, left_on=['UPC'], right_on=['UPC'], how ='inner').drop_duplicates(keep='first')\n",
    "    Itembridge = itembridgedf[['IADK','ILDK']]\n",
    "    Itembridge.rename(index = str, columns={'IADK':'ItemAttribute_IADK','ILDK':'ItemList_ILDK'})\n",
    "    ItemattributesDim.to_sql('ItemattributesDim', engine, if_exists='append', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date and time dimensions created throught transaction time in sales fact table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Date Dimension\n",
    "    temp = pd.DatetimeIndex(saleDF['TransDatetime(GMT)'])\n",
    "    saleDF['Year_int'] = temp.year\n",
    "    saleDF['Month_int'] = temp.month\n",
    "    saleDF['Month_abbr'] = saleDF['Month_int'].apply(lambda x: calendar.month_abbr[x])\n",
    "    saleDF['Day_int'] = temp.day\n",
    "    saleDF['DayOfWeek_int'] = temp.dayofweek\n",
    "    saleDF['DayOfWeek_char'] = saleDF['DayOfWeek_int'].apply(lambda x: calendar.day_name[x])\n",
    "    saleDF['DayOfYear_int'] = temp.dayofyear\n",
    "    saleDF['Date'] = saleDF['TransDatetime(GMT)']\n",
    "    datedim = saleDF[['TransDatetime(GMT)','Date','Year_int','Month_int','Month_abbr','Day_int','DayOfWeek_int','DayOfWeek_char','DayOfYear_int']].drop_duplicates(keep='first')\n",
    "    datedim[['Date','Year_int','Month_int','Month_abbr','Day_int','DayOfWeek_int','DayOfWeek_char',\n",
    "            'DayOfYear_int']].to_sql('DateDim', engine, if_exists='append',index=False)\n",
    "\n",
    "    #Time Dimension\n",
    "    saleDF['Time_hhmmss_char'] = temp.time.astype(str)\n",
    "    saleDF['Hour_24_int'] = temp.hour\n",
    "    saleDF['Time'] =saleDF['Time_hhmmss_char']\n",
    "    saleDF['Minute_int'] = temp.minute\n",
    "    saleDF['Second_int'] = temp.second\n",
    "    temp_12hour = saleDF['TransDatetime(GMT)'] + timedelta(hours=12)\n",
    "    saleDF['Hour_12_int'] = pd.DatetimeIndex(temp_12hour).hour\n",
    "    saleDF['AM_PM_char'] = saleDF['Hour_12_int']\n",
    "    i=0\n",
    "    ampm = []\n",
    "    for i in list(range(0,24)):\n",
    "        if(i< 12):\n",
    "            ampm.append('AM')\n",
    "            i=i+1\n",
    "        elif(i>=12):\n",
    "            ampm.append('PM')\n",
    "            i=i+1\n",
    "    mapping = dict(zip(list(range(0,24)),ampm))\n",
    "    saleDF.replace({'AM_PM_char': mapping})\n",
    "    #print(saleDF)\n",
    "\n",
    "    timedim = saleDF[['TransDatetime(GMT)','Time','Hour_24_int','Minute_int','Second_int','Hour_12_int','AM_PM_char']].drop_duplicates(keep='first')\n",
    "\n",
    "    timedim.rename(index = str,columns ={'Time':'Time_hhmmss_char','Hour_24_int':'Hour_24_int','Minute_int':'Minute_int','Second_int':'Second_int','Hour_12_int':'Hour_12_int','AM_PM_char':'AM_PM_char'})\n",
    "    timedim[['Time','Hour_24_int','Minute_int','Second_int','Hour_12_int','AM_PM_char']].to_sql('TimeDim', engine, if_exists='append', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sales junk dimension loaded from sale dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Sales Junk Dimension records are inserted\n",
    "    salejunkdimtable = salejunkDF[['Register','DeptNum','CashierNum','PriceType','ServiceType','TenderType']].drop_duplicates(keep='first')\n",
    "    salejunkdimtable.to_sql('salesjunkdim', engine, if_exists='append', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FACT table loaded by merging the sales DF and each dimension table to filter the dimension keys and load the dimension keys to the sales transaction fact table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting sales junk dimension keys to salesfact dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Inserting FACT records in FACT table for sales transaction data\n",
    "\n",
    "    salesfact = saleDF[['UPC','ItemID','TransDatetime(GMT)','StoreNum','WeightAmt','SalesAmt','BusDate','TransNum','ItemQuantity','CostAmt','Register','DeptNum','CashierNum','PriceType','ServiceType','TenderType']]\n",
    "\n",
    "    salesjunktable = pd.read_sql_table('salesjunkdim', engine)\n",
    "    salesjunkfact = pd.merge(salejunkDF[['StoreNum','Register','DeptNum','CashierNum','PriceType','ServiceType','TenderType']], salesjunktable, left_on=['Register','DeptNum','CashierNum','PriceType','ServiceType','TenderType'], right_on=['Register','DeptNum','CashierNum','PriceType','ServiceType','TenderType'], how ='inner').drop_duplicates(keep='first')\n",
    "\n",
    "    salesfact =    pd.merge(salesfact[['UPC','ItemID','TransDatetime(GMT)','WeightAmt','SalesAmt','BusDate','TransNum','ItemQuantity','CostAmt','Register','DeptNum','CashierNum','PriceType','ServiceType','TenderType','StoreNum']], salesjunkfact[['SJDK','StoreNum','Register','DeptNum','CashierNum','PriceType','ServiceType','TenderType']], left_on=['StoreNum','Register','DeptNum','CashierNum','PriceType','ServiceType','TenderType'], right_on=['StoreNum','Register','DeptNum','CashierNum','PriceType','ServiceType','TenderType'], how ='inner').drop_duplicates(keep='first')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting item list dimension keys to itemfact dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    itemfact = pd.read_sql_table('itemlistdim', engine, columns=['ILDK', 'UPC','ItemID'])\n",
    "    itemsalesfact = pd.merge(itemfact, salesfact, left_on=['UPC','ItemID'], right_on=['UPC','ItemID'], how ='inner').drop_duplicates(keep='first')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date and time dimension keys to datefact and timefact dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    datefact = pd.read_sql_table('datedim', engine, columns=['DDK','Date'])\n",
    "    itemsalesdatefact = pd.merge(itemsalesfact, datefact, left_on=['TransDatetime(GMT)'], right_on=['Date'], how ='inner').drop_duplicates(keep='first')\n",
    "    itemsalesdatefact['time'] = pd.DatetimeIndex(itemsalesdatefact['Date']).time\n",
    "\n",
    "    timefact = pd.read_sql_table('timedim', engine, columns=['TDK','Time'])\n",
    "    itemsalesdatetimefact = pd.merge(itemsalesdatefact, timefact, left_on=['time'], right_on=['Time'], how ='inner').drop_duplicates(keep='first')\n",
    "\n",
    "    itemsaledtfact = itemsalesdatetimefact\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "item junk dimension keys to itemjunkfact dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    itemjunktable = pd.read_sql_table('itemjunkdim', engine, columns=['IJDK','StoreBrand','Status'])\n",
    "    itemjunk = pd.merge(itemjunktable, item_sale[['StoreBrand','Status','UPC','ItemID','ClassCode','CategoryCode','DepartmentCode','FamilyCode']], left_on=['StoreBrand','Status'], right_on=['StoreBrand','Status'], how ='inner').drop_duplicates(keep='first')\n",
    "    itemjunkfact = pd.merge(itemsaledtfact, itemjunk, left_on=['UPC','ItemID'], right_on=['UPC','ItemID'], how ='inner').drop_duplicates(keep='first')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aligning the columns from dataframes and dimension tables to a common data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    allitemsalesdtfact=itemjunkfact\n",
    "    itemtemp_sale=item_sale\n",
    "    allitemsalesdtfact['sales_SJDK'] = allitemsalesdtfact['SJDK']\n",
    "\n",
    "\n",
    "    itemtemp_sale['ClassCode']=itemtemp_sale['ClassCode'].astype(str)\n",
    "    itemtemp_sale['CategoryCode']=itemtemp_sale['CategoryCode'].astype(str)\n",
    "    itemtemp_sale['DepartmentCode']=itemtemp_sale['DepartmentCode'].astype(str)\n",
    "    itemtemp_sale['FamilyCode']=itemtemp_sale['FamilyCode'].astype(str)\n",
    "    allitemsalesdtfact['ClassCode']=allitemsalesdtfact['ClassCode'].astype(str)\n",
    "    allitemsalesdtfact['CategoryCode']=allitemsalesdtfact['CategoryCode'].astype(str)\n",
    "    allitemsalesdtfact['DepartmentCode']=allitemsalesdtfact['DepartmentCode'].astype(str)\n",
    "    allitemsalesdtfact['FamilyCode']=allitemsalesdtfact['FamilyCode'].astype(str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "item hierarchy dimension key to itemhierfact dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    itemhiertable = pd.read_sql_table('itemhierarchydim', engine, columns=['IHDK','ClassCode','CategoryCode','DepartmentCode','FamilyCode'])\n",
    "\n",
    "    itemhiertemp = pd.merge(itemhiertable, itemtemp_sale, left_on=['ClassCode','CategoryCode','DepartmentCode','FamilyCode'], right_on=['ClassCode','CategoryCode','DepartmentCode','FamilyCode'], how ='inner').drop_duplicates(keep='first')\n",
    "    itemhierfact = pd.merge(allitemsalesdtfact[['sales_SJDK','ILDK','UPC','ItemID','TransDatetime(GMT)','StoreNum','WeightAmt','SalesAmt','BusDate','TransNum','ItemQuantity','CostAmt','DDK','Date','TDK','IJDK','ClassCode','CategoryCode','DepartmentCode','FamilyCode']], itemhiertemp[['IHDK','ClassCode','CategoryCode','DepartmentCode','FamilyCode']], left_on=['ClassCode','CategoryCode','DepartmentCode','FamilyCode'], right_on=['ClassCode','CategoryCode','DepartmentCode','FamilyCode'], how ='inner').drop_duplicates(keep='first')\n",
    "\n",
    "    halffact = itemhierfact\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "store location dimension key and store junk dimension keys to storefact and storejunkfact dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    storetable = pd.read_sql_table('storelocationdim', engine, columns=['SLDK','Region','StateCode','City','ZipCode','AddressLine1'])\n",
    "    storetablefact = pd.merge(storeDF[['StoreName','ClusterName','StoreNum','Region','StateCode','City','ZipCode','AddressLine1']],storetable[['SLDK','Region','StateCode','City','ZipCode','AddressLine1']],left_on=['Region','StateCode','City','ZipCode','AddressLine1'] ,right_on=['Region','StateCode','City','ZipCode','AddressLine1'],how='inner').drop_duplicates(keep='first')\n",
    "    storefact = pd.merge(halffact[['sales_SJDK','IHDK','ILDK','UPC','ItemID','TransDatetime(GMT)','StoreNum','WeightAmt','SalesAmt','BusDate','TransNum','ItemQuantity','CostAmt','DDK','Date','TDK','IJDK','ClassCode','CategoryCode','DepartmentCode','FamilyCode']], storetablefact[['StoreName','ClusterName','StoreNum','SLDK','Region','StateCode','City','ZipCode','AddressLine1']], left_on=['StoreNum'], right_on=['StoreNum'], how ='inner').drop_duplicates(keep='first')\n",
    "    storejunktable = pd.read_sql_table('storejunkdim', engine, columns=['SJDK','StoreName','ClusterName','StoreNum','ActiveFlag','SqFoot'])\n",
    "\n",
    "    storejunkfact = pd.merge(storefact[['sales_SJDK','SLDK','IHDK','ILDK','UPC','ItemID','ClusterName','StoreName','TransDatetime(GMT)','StoreNum','WeightAmt','SalesAmt','BusDate','TransNum','ItemQuantity','CostAmt','DDK','Date','TDK','IJDK','ClassCode','CategoryCode','DepartmentCode','FamilyCode']], storejunktable[['StoreName','ClusterName','StoreNum','SJDK','ActiveFlag','SqFoot']], left_on=['StoreName','ClusterName','StoreNum'], right_on=['StoreName','ClusterName','StoreNum'], how ='inner').drop_duplicates(keep='first')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "store service dimension keys to prefinalfact dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    storeservicetable = pd.read_sql_table('storeservicedim', engine)\n",
    "    prefinalfact = pd.merge(storejunkfact, storeservicetable, left_on=['StoreNum'], right_on=['StoreNum'], how ='inner').drop_duplicates(keep='first')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "customer dimension key to finalfact dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    prefinalfact= prefinalfact[['SSDK','SJDK','sales_SJDK','SLDK','IHDK','ILDK','UPC','ItemID','ClusterName','StoreName','TransDatetime(GMT)','StoreNum','WeightAmt','SalesAmt','BusDate','TransNum','ItemQuantity','CostAmt','DDK','Date','TDK','IJDK','ClassCode','CategoryCode','DepartmentCode','FamilyCode']]\n",
    "    customertable = pd.read_sql_table('customerdim', engine)\n",
    "    customerfact = pd.merge(saleDF[['LoyaltyCardNumber']], customertable, left_on=['LoyaltyCardNumber'], right_on=['LoyaltyCardNum'], how ='inner').drop_duplicates(keep='first')\n",
    "\n",
    "    prefinalfact = pd.merge(saleDF[['TransNum','LoyaltyCardNumber']], prefinalfact[['SSDK','sales_SJDK','SJDK','SLDK','IHDK','ILDK','UPC','ItemID','ClusterName','StoreName','TransDatetime(GMT)','StoreNum','WeightAmt','SalesAmt','BusDate','TransNum','ItemQuantity','CostAmt','DDK','Date','TDK','IJDK','ClassCode','CategoryCode','DepartmentCode','FamilyCode']], left_on=['TransNum'], right_on=['TransNum'], how ='inner').drop_duplicates(keep='first')\n",
    "\n",
    "    finalfact =  pd.merge(prefinalfact[['SSDK','LoyaltyCardNumber','sales_SJDK','SJDK','SLDK','IHDK','ILDK','UPC','ItemID','ClusterName','StoreName','TransDatetime(GMT)','StoreNum','WeightAmt','SalesAmt','BusDate','TransNum','ItemQuantity','CostAmt','DDK','Date','TDK','IJDK','ClassCode','CategoryCode','DepartmentCode','FamilyCode']], customerfact[['CDK','LoyaltyCardNum']], left_on=['LoyaltyCardNumber'], right_on=['LoyaltyCardNum'], how ='inner').drop_duplicates(keep='first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loading data to the fact table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    fact = finalfact[['SSDK','CDK','sales_SJDK','SJDK','SLDK','IHDK','ILDK','UPC','ItemID','ClusterName','StoreName','TransDatetime(GMT)','StoreNum','WeightAmt','SalesAmt','BusDate','TransNum','ItemQuantity','CostAmt','DDK','Date','TDK','IJDK','ClassCode','CategoryCode','DepartmentCode','FamilyCode']]\n",
    "\n",
    "    fact.rename(index = str, columns={'CustomerDim_CDK':'CDK','SalesJunkDim_SJDK':'sales_SJDK','StoreJunkDim_SJDK':'SJDK','StoreServiceDim_SSDK':'SSDK','StoreLocationDim_SLDK':'SLDK','ItemHierarchyDim_IHDK':'IHDK','ItemJunkDim_IJDK':'IJDK','ItemListDim_ILDK':'ILDK','TimeDim_TDK':'TDK','DateDim_DDK':'DDK'})\n",
    "\n",
    "    fact[['CustomerDim_CDK']]=fact[['CDK']]\n",
    "    fact[['SalesJunkDim_SJDK']]=fact[['sales_SJDK']]\n",
    "    fact[['StoreJunkDim_SJDK']]=fact[['SJDK']]\n",
    "    fact[['StoreServiceDim_SSDK']]=fact[['SSDK']]\n",
    "    fact[['StoreLocationDim_SLDK']]=fact[['SLDK']]\n",
    "    fact[['ItemHierarchyDim_IHDK']]=fact[['IHDK']]\n",
    "    fact[['ItemJunkDim_IJDK']]=fact[['IJDK']]\n",
    "    fact[['ItemListDim_ILDK']]=fact[['ILDK']]\n",
    "    fact[['TimeDim_TDK']]=fact[['TDK']]\n",
    "    fact[['DateDim_DDK']]=fact[['DDK']]\n",
    "    fact[['WeightAmt']] = fact[['WeightAmt']].round(1)\n",
    "\n",
    "    fact = fact[['CustomerDim_CDK','SalesJunkDim_SJDK','StoreJunkDim_SJDK','StoreServiceDim_SSDK','StoreLocationDim_SLDK','ItemHierarchyDim_IHDK','ItemJunkDim_IJDK','ItemListDim_ILDK','TimeDim_TDK','DateDim_DDK','BusDate','TransNum','ItemQuantity','WeightAmt','SalesAmt','CostAmt']]\n",
    "\n",
    "    fact.to_sql('trans_fact', engine, if_exists='append', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
